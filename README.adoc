= Serverless and Pipelines
:experimental:
:git-repo: https://github.com/kameshsampath/minikube-helpers

== Pre-requisites
* https://github.com/mikefarah/yq/
* https://github.com/ahmetb/kubectx/blob/master/kubens[kubens]
* https://github.com/tektoncd/cli[tkn]

== Minikube 

[source,bash]
----
minikube start --memory=8192 --cpus=6 --disk-size=50G  \
 --kubernetes-version=v1.12.0 \
 --vm-driver=hyperkit \
 --extra-config=apiserver.enable-admission-plugins="LimitRanger,NamespaceExists,NamespaceLifecycle,ResourceQuota,ServiceAccount,DefaultStorageClass,MutatingAdmissionWebhook"
----

[IMPORTANT]
====
This setup and example works well with minikube v1.1.1, the registry hack below is not working as expected with v1.2.0.
====

== Enable registry addon 

IMPORTANT: Only for minikube 

[source,bash]
----
minikube addons enable registry
----

Wait for the registry pod to be up 

[source,bash]
----
kubectl -n kube-system get pods -w 
----

NOTE: You can terminate the command with kbd:[CTRL+c]

Clone the minikube-helpers repo

[source,bash,subs="attribute+,macros+"]
----
git clone {git-repo}
cd minikube-helpers
----

== Configure registry aliases

To be able to push and pull images from internal registry we need to make the registry entry in minikube node's **hosts** file and make them resolvable via **coredns**. 

=== Add entries to host file

All the registry aliases are configured using the configmap `registry-aliases-config.yaml`, we need to create the configmap in `kube-system` namespace:

[source,bash]
----
git clone https://github.com/kameshsampath/minikube-helpers
cd registry
kubectl apply -n kube-system -f registry-aliases-config.yaml
----

Once the ConfigMap has been created we can run the dameonset `node-etc-hosts-update.yaml` to make in add entries to the minikube node's `/etc/hosts` file with all aliases pointing to internal registrys' __CLUSTER_IP__

[source,bash]
----
kubectl apply -n kube-system -f node-etc-hosts-update.yaml
----

[NOTE]
====
 * Wait for the daemonset to be running before proceeding to next step, the status of the daemonset can be viewed via `kubectl get pods -n kube-system -w`, you can do kbd:[CTRL+C] to end the watch.
 * The daemonset may take few minutes to run and complete.
====

You can check the minikube vm's `/etc/hosts` file for the registry aliases entries:

[source,bash]
----
$ minikube ssh -- sudo cat /etc/hosts
127.0.0.1       localhost
127.0.1.1 demo
10.111.151.121  dev.local
10.111.151.121  example.com
----
The above output shows that the daemonset has added the `registryAliases` from the ConfigMap pointing to the internal registry's __CLUSTER-IP__.

=== Update coredns

Update the Kubernetes' coredns to have rewrite rules for aliases.

[source,bash]
----
./patch-coredns.sh
----

A successful patch will have the coredns configmap updated like:

[source,yaml]
----
apiVersion: v1
data:
  Corefile: |-
    .:53 {
        errors
        health
        rewrite name dev.local registry.kube-system.svc.cluster.local
        rewrite name example.com registry.kube-system.svc.cluster.local
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           upstream
           fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
kind: ConfigMap
metadata:
  name: coredns
----

To verify it run the following command:

[source,bash]
----
kubectl get cm -n kube-system coredns -o yaml
----

Once you have successfully patched you can now push and pull from the registry using suffix `dev.local`, `example.com`

== Install Tekton Pipelines

[source,bash]
----
kubectl apply --filename https://storage.googleapis.com/tekton-releases/latest/release.yaml
----

Wait for the Tekton Pipelines Pods to come up 

[source,bash]
----
kubectl get pods --namespace tekton-pipelines -w 
----

NOTE: You can terminate the command with kbd:[CTRL+c]

== Install Knative Serving 

NOTE: This section is optional, if and only if you wish to deploy Knative services

[source,bash]
----
curl -L  https://raw.githubusercontent.com/knative/serving/release-0.6/third_party/istio-1.1.3/istio-lean.yaml \
  | sed 's/LoadBalancer/NodePort/' \
  | kubectl apply --filename -
----

Wait for the Istio Pods to come up 

[source,bash]
----
kubectl get pods --namespace isito-system -w 
----

NOTE: You can terminate the command with kbd:[CTRL+c]

[source,bash]
----
kubectl apply --selector knative.dev/crd-install=true \
--filename https://github.com/knative/serving/releases/download/v0.6.0/serving.yaml \
--filename https://github.com/knative/serving/releases/download/v0.6.0/serving.yaml --selector networking.knative.dev/certificate-provider!=cert-manager 
----

Wait for the  Knative Serving Pods to come up 

[source,bash]
----
kubectl get pods --namespace knative-serving -w 
----

NOTE: You can terminate the command with kbd:[CTRL+c]

== Configure Pipelines

As the build need to be run with service account that needs permissions to create resources, a new service account 'build-robot' needs to be created with required permissions.

Download the demo sources and lets call the folder as `$PROJECT_HOME`:

[source,bash]
----
git clone https://github.com/redhat-developer-demos/quarkus-pipeline-demo &&\
cd quarkus-pipeline-demo &&\
export PROJECT_HOME=`pwd`
----

IMPORTANT: All the objects will be created in the namespace called `demos`, if you wish to change it please edit the file build/build-roles.yaml and update the namespace name.

[source,bash]
----
kubectl apply -f $PROJECT_HOME/build/build-roles.yaml
----

Change to the `demos` namespace 

[source,bash]
----
kubens demos
----

The build uses resources called https://github.com/tektoncd/pipeline/blob/master/docs/resources.md[PipelineResource] that helps to configure the git repo url, the final container image name etc., 

Let's create the resources

[source,bash]
----
kubectl apply -f $PROJECT_HOME/build/build-resources.yaml
----

The Pipeline consists of multiple tasks that needs to be executed in order.

Let's create the pipeline tasks

[source,bash]
----
kubectl apply --recursive -f $PROJECT_HOME/build/tasks
----

You can use the command `tkn task list`  to list the created tasks. The command above should show the following tasks:

```
NAME                     AGE
greeter-image-from-git   22 seconds ago
kubectl-task             22 seconds ago
```

Let's create the pipeline that uses the tasks create in previous step

[source,bash]
----
kubectl apply --recursive -f $PROJECT_HOME/build/pipelines
----

You can use the command `tkn pipeline list`  to list the created tasks. The command above should show the following pipeline:

```
NAME                      AGE             LAST RUN   STARTED   DURATION   STATUS
greeter-pipeline-jvm      5 seconds ago   ---        ---       ---        ---
greeter-pipeline-native   5 seconds ago   ---        ---       ---        ---
```

To make the pipeline run, we need to create the  https://github.com/tektoncd/pipeline/blob/master/docs/pipelineruns.md[PipelineRun]

Let's create the pipelinerun that uses the one of pipelines e.g. **greeter-pipeline-jvm** created in previous step

[source,bash]
----
kubectl apply  -f $PROJECT_HOME/build/pipelinerun/greeter-pipeline-run.yaml
----

[TIP]
====
If you want to do a native build then update the `pipelineRef` in `greeter-pipeline-run.yaml` to be `greeter-pipeline-native`
====

You can use the command `tkn pipelinerun list`  to list the created tasks. The command above should show the following pipeline:

```
NAME                   STARTED         DURATION   STATUS
greeter-pipeline-run   8 seconds ago   ---        Running
```

You can view the logs of the pipeline run using the command `tkn pipelinerun logs -f -a greeter-pipeline-run`

NOTE: The very first pipeline run may take sometime, as the builder images needs to be downloaded and the maven cache needs to be warmed

[TIP]
====
If you have a local maven repo manager like https://www.sonatype.com/nexus-repository-sonatype[Nexus] then you can configure the pipeline to use it via the param `mavenMirrorUrl`
e.g.
[source,yaml]
----
  params:
    - name: mavenMirrorUrl
      value: http://192.168.99.1:8081/nexus/content/groups/public #<1>
----
<1> Assuming your nexus repository is running in http://192.168.99.1:8081
====

A successful pipeline run will deploy an application called "greeter" and a correponding service called `greeter-service`, you can view them using the following commands:

[source,bash]
----
kubectl get -n demos deployments
kubectl get -n demos services
----

== Deploying Knative Service

To deploy Knative service using the same pipelines, edit the ./build/pipelinerun/greeter-pipeline-run.yaml and update it to look like:

[source,yaml]
----
apiVersion: tekton.dev/v1alpha1
apiVersion: tekton.dev/v1alpha1
kind: PipelineRun
metadata:
  name: greeter-pipeline-run
spec:
  serviceAccount: build-robot
  pipelineRef:
    name: greeter-pipeline-jvm
  params:
    - name: namespace
      value: demos
    - name: resourceDir #<1>
      default: "knative"
    - name: resourceFile
      default: "service.yaml" #<2>
  resources:
    - name: source-repo
      resourceRef:
        name: demo-git-source
    - name: app-container-image
      resourceRef:
        name: greeter-local-image-jvm
----
<1> The kubernetes resource directory
<2> The Knative service yaml file 

Recreate the pipelines

[source,bash]
----
kubectl delete -f $PROJECT_HOME/build/pipelinerun/greeter-pipeline-run.yaml && \
kubectl apply -f $PROJECT_HOME/build/pipelinerun/greeter-pipeline-run.yaml
----

=== Invoke Service
[source,bash]
----
IP_ADDRESS="$(minikube ip):$(kubectl get svc istio-ingressgateway --namespace istio-system --output 'jsonpath={.spec.ports[?(@.port==80)].nodePort}')"
curl -H "host:greeter.demos.example.com" $IP_ADDRESS/greeter
----

== Cleanup 
[source,bash]
----
kubectl delete --recursive -f $PROJECT_HOME/build
kubens - 
----
